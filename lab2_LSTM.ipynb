{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Strategy for Finance using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab was developed by Mike Imas, Onur Yilmaz Ph.D., and Andy Steinbach Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's verify WebSockets are working on your system. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Shift-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The answer should be three: \" + str(1+2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the cell below to display information about the GPUs running on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lab Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab focuses on the prediction of time series  financial data using a special recurrent neural network (RNN), called Long Short Term Memory (LSTM), for trading strategies in finance. \n",
    "\n",
    "The goal of this lab is to give you a deep learning (DL) approach that can be potentially beneficial to the complex trading strategies in finance. This lab is not a complete trading strategy that generates a profit and loss curve (PNL). Rather, it shows how LSTM based deep neural networks can be applied to predict time series financial data. The code provided in this lab can be repurposed to predict the any time series financial data to be used to make certain decisions, opening a long position or closing a short position.\n",
    "\n",
    "DL has been disrupting many applications including computer vision, natural language processing, and there has been a flurry of research and development activities in different verticals of the industry such as healthcare and finance to exploit this new technology for the area specific use cases. DL based investment strategies are also in the center of research and development activities in the algorithmic trading. \n",
    "\n",
    "In this lab, it is assumed that you are familiar with RNNs, TensorFlow, and Python. For more information on RNNs, LSTMs, and TensorFlow, please check the relevant labs in DLI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation of LSTM based Financial Data Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will go over the implementation of Long Short Term Memory (LSTM) based financial data predictor using a dataset from Kaggle provided by Two Sigma Investment, which is a New York City based hedge fund company. The reason we picked this dataset is to use expertly generated features for training and inference. The goal of this course is to give the experience of how LSTMs can be applied to predict time series financial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Long Short Term Memory (LSTM) Networks?\n",
    "\n",
    "LSTM is a variant of recurrent neural network (RNN) and was published by Hochreiter & Schmidhuber in 1997. RNNs are an extension of regular artificial neural networks that add connections feeding the hidden state of the neural network back into itself, these are called recurrent connections. The reason for adding these recurrent connections is to provide the network with visibility not just of the current data sample it has been provided, but also it's previous hidden state. In some sense, this gives the network a sequential memory of what it has seen before. This makes RNNs applicable in situations where a sequence of data (like in most of the financial data) is required to make a classification decision or regression estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn.jpg\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                    Figure 1: Recurrent Neural Networks (RNNs)\n",
    "                                    \n",
    "LSTMs do not have vanishing gradient problems like in most of the RNNs. LSTM is normally augmented by recurrent gates called forget gates. As mentioned, a defining feature of the LSTM is that it prevents backpropagated errors from vanishing (or exploding) and instead allow errors to flow backwards through unlimited numbers of \"virtual layers\" unfolded in time. That is, the LSTM can learn \"very deep\" tasks that require memories of events that happened thousands or even millions of discrete time steps ago. Problem-specific LSTM-like topologies can be evolved and can work even when signals contain long delays or have a mix of low and high frequency components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Financial Terminologies\n",
    "\n",
    "Before we start running the code, we include some of the financial terminologies that we use in this lab. The definitions are taken directly from Investopedia.com.\n",
    "\n",
    "**Trading Strategy:** A set of objective rules defining the conditions that must be met for a trade entry and exit to occur. Trading strategies include specifications for trade entries, including trade filters and triggers, as well as rules for trade exits, money management, timeframes, order types, and other relevant information. A trading strategy, if based on quantifiably specifications, can be analyzed based on historical data to project future performance.\n",
    "\n",
    "**Instrument:** An instrument is a tradeable asset or negotiable item such as a security, commodity, derivative or index, or any item that underlies a derivative. An instrument is a means by which something of value is transferred, held or accomplished.\n",
    "\n",
    "**Security:** It is a fungible, negotiable financial instrument that holds some type of monetary value. It represents an ownership position in a publicly-traded corporation (via stock), a creditor relationship with a governmental body or a corporation (represented by owning that entity's bond), or rights to ownership as represented by an option.\n",
    "\n",
    "**Stock**: A stock is a type of security that signifies ownership in a corporation and represents a claim on part of the corporation's assets and earnings. It is delivered in the units of shares.\n",
    "\n",
    "**Share**: Shares are units of ownership interest in a corporation or financial asset.\n",
    "\n",
    "**Long Position (Long)**: A long (or long position) is the buying of a security such as a stock, commodity or currency with the expectation that the asset will rise in value. Trader normally has no plan to sell the security in the near future. A key component of long position investment is the ownership of the stock or bond.\n",
    "    \n",
    "**Short Position (Short)**: A short, or short position, is a directional trading or investment strategy where the investor sells shares of borrowed stock in the open market. The expectation of the investor is that the price of the stock will decrease over time, at which point the he will purchase the shares in the open market and return the shares to the broker which he borrowed them from.\n",
    "\n",
    "**Return**: A return is the gain or loss of a security in a particular period. The return consists of the income and the capital gains relative on an investment, and it is usually quoted as a percentage. The general rule is that the more risk you take, the greater the potential for higher returns and losses.\n",
    "\n",
    "**Fundamental Analysis:** It is a method of evaluating a security in an attempt to measure its intrinsic value, by examining related economic, financial and other qualitative and quantitative factors. Fundamental analysts study anything that can affect the security's value, including macroeconomic factors such as the overall economy and industry conditions, and microeconomic factors such as financial conditions and company management. For instance, for stocks and equity instruments, this method uses revenues, earnings, future growth, return on equity, profit margins and other data to determine a company's underlying value and potential for future growth.\n",
    "\n",
    "**Technical Analysis:** It is the evaluation of securities by means of studying statistics generated by market activity, such as past prices and volume. Technical analysts do not attempt to measure a security's intrinsic value but instead use stock charts to identify patterns and trends that may suggest what a stock will do in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Two Sigma (2$\\sigma$) Investment Dataset in Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In December 2016, Two Sigma Investments, a New York City based hedge fund company, announced a Kaggle challenge called the [**Two Sigma Financial Modeling Challenge**](https://www.kaggle.com/c/two-sigma-financial-modeling) with the prize pool of $100,000. Two Sigma's goal is to explore what untapped value Kaggle's diverse data science community can discover in the financial markets.\n",
    "\n",
    "This dataset that was published on Kaggle contains fundamental and technical features pertaining to a time-varying value for a financial instrument. These features are generated by fundamental and technical analysis. Variable to predict is \"y\" which is the return of an instrument. Features and \"y\" variable are anonymized by using special transformations like principal component analysis (PCA) in order to protect the original data. Each instrument has an id and time is represented by the 'timestamp' feature. We picked this dataset because it includes expertly generated feature set for training and inference. Structure of the data is depicted in Figure 2. Some of the features are as follows;\n",
    "   \n",
    "**Fundamental Features:** Macroeconomic factors (overall economy, industry conditions, financial conditions), revenues, earnings, future growth, profit margins, etc.\n",
    "  \n",
    "**Technical Features:** Price movements, analytical and statistical tools like mean, standard deviation, moving averages, etc.\n",
    "\n",
    "**\"y\" scalar variable:** Return of the instrument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/data.jpg\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                      Figure 2: Two Sigma Investment Dataset\n",
    "\n",
    "Data is saved and accessed as a HDF5 file in the Kernels environment. [HDF5](http://www.hdfgroup.org/) stands for hierarchical data format version number 5.  The HDF format is designed specifically to store and organize large amounts of scientific data. Common file extensions include ```.hdf```, ```.hdf5```, or simply ```.h5```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### c. Step by Step Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical DL workflow starts with data preparation because the data is not clean and ready to use most of the time. Deep neural network building and training follow the data preparation. Lastly, the trained network is validated with a dataset. \n",
    "\n",
    "First, we import several widely used modules such as NumPy for numerical calculations, pandas for data management, matplotlib for visualizations, and TensorFlow for building and training deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import h5py\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pprint as pp \n",
    "import tensorflow as tf \n",
    "from tensorflow.contrib import rnn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import prepareData as prepData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data needs to be cleaned before training the network. Since cleaning the data takes significant amount of time (around 20 minutes), we have stored the cleaned data into another .h5 file. If you would like to use the original data and run the cleaning code, please set the \"usePreparedData\" variable to \"False\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is prepared and stored in a seperate .h5 file.\n",
    "# Set usePreparedData = False to use the original data and run the data preparation code\n",
    "usePreparedData = True\n",
    "# insampleCutoffTimestamp variable is used to split the data in time into two pieces to create training and test set.\n",
    "insampleCutoffTimestamp = 1650\n",
    "\n",
    "# If usePreparatedData is True, then the prepared data is stored. Otherwise, the original data is stored\n",
    "if usePreparedData == True:\n",
    "    #with pd.HDFStore(\"/home/mimas/2sigma/DLI_FSI/2sigma/train_prepared.h5\", 'r') as train:\n",
    "    with pd.HDFStore(\"/dli/data/algo_trading/trainDataPrepared.h5\", 'r') as train:\n",
    "        df = train.get(\"train\") \n",
    "else:\n",
    "    with pd.HDFStore(\"/dli/data/algo_trading/train.h5\", 'r') as train:\n",
    "        df = train.get(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple instruments in the dataset and each instrument has an id. Time is represented by the 'timestamp' feature. Let's look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will print the dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original data is stored, the data preparation code will be executed in the following cell. First, extreme values in each feature set are removed. Then, some hand-crafted features are added to feature set to boost the prediction accuracy. There are many methods including PCA and auto-encoders to do the feature engineering rather than creating hand-crafted features. As an exercise, we highly recommend you to add auto-encoders to the code and check the accuracy after the lab. Lastly, NaNs are replaced with the median of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if usePreparedData == False:\n",
    "    # Original data is not clean and some the samples are a bit extreme.\n",
    "    # These values are removed from the feature set.\n",
    "    df = prepData.removeExtremeValues(df, insampleCutoffTimestamp)\n",
    "    # A little bit feature engineering. Hand-crafted features are created here to boost the accuracy.\n",
    "    df = prepData.createNewFeatures(df) \n",
    "    # Check whether ve still have any NaNs \n",
    "    df = prepData.fillNaNs(df) \n",
    "    df.to_hdf(\"/dli/data/algo_trading/trainDataPrepared.h5\", 'train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the TensorFlow compute graph. The deep neural network that is used in this code is comprised of a LSTM cell that runs over 10 time steps, a fully connected layers (FCL), and also drop-out layers to prevent overfitting. Calculating the number of time steps for a recurrent neural network is not a trivial task. It is actually another hyperparameter that needs to be searched. The network is depicted in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dnn.jpg\" width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                Figure 3: Structure of the LSTM based deep neural network\n",
    "                                \n",
    "Below is the code to build the deep neural network depicted in Figure 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape): \n",
    "    initial = tf.truncated_normal(shape, stddev=0.3)\n",
    "    return tf.Variable(initial) \n",
    "    \n",
    "def bias_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.3)\n",
    "    return tf.Variable(initial) \n",
    "\n",
    "n_time_steps = 10\n",
    "def getDNN (x, LSTMCellSize, keep_prob):\n",
    "    with tf.name_scope('model'):\n",
    "        with tf.name_scope('RNN'):\n",
    "            # We will add two dropout layers and LSTM cells with the number of units as LSTMCellSize.\n",
    "            cell = rnn.DropoutWrapper(rnn.BasicLSTMCell(LSTMCellSize, forget_bias=2, activation=tf.nn.tanh), output_keep_prob=keep_prob)\n",
    "            # We use the cell to create RNN.\n",
    "            # Note that outputs is not a tensor, it is a list with one element which is numpy array. \n",
    "            outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32) \n",
    "            outputs_shape = outputs.get_shape().as_list()\n",
    "                \n",
    "        # hidden layer with sigmoid activation\n",
    "        with tf.name_scope('W_fc1'):\n",
    "            W_fc1 = weight_variable([LSTMCellSize, 1])\n",
    "        with tf.name_scope('b_fc1'):\n",
    "            b_fc1 = bias_variable([1])\n",
    "        with tf.name_scope('pred'):\n",
    "            pred = tf.matmul(outputs[:,-1,:], W_fc1) + b_fc1\n",
    "\n",
    "        return pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column names that will be included in the featureset are added into colList.\n",
    "# colList will be used throughout the lab.\n",
    "colList=[]                  \n",
    "for thisColumn in df.columns: \n",
    "    if thisColumn not in ('id', 'timestamp', 'y', 'CntNs', 'y_lagged'): \n",
    "        colList.append(thisColumn)\n",
    "colList.append('y_lagged')\n",
    "\n",
    "#if you do not reset the default graph you will need to restart the kernel\n",
    "#every time this notebook is run\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Network Parameters \n",
    "# Number of units in the LSTM cell.\n",
    "n_LSTMCell = len(colList)\n",
    "\n",
    "# Placeholder for the input and the keep probability for the dropout layers\n",
    "with tf.name_scope('input'):\n",
    "    x= tf.placeholder(tf.float32, shape=[None, n_time_steps, len(colList)])\n",
    "with tf.name_scope('keep_prob'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# At the input, we create 2-layer LSTM cell (with dropout layers)\n",
    "print('Building tensorflow graph')\n",
    "\n",
    "# Graph construction for the LSTM based deep neural network. \n",
    "# Structure of the network is depicted in the above figure.\n",
    "# Please see the dnn.py to see the code of the network.\n",
    "pred = getDNN (x, n_LSTMCell, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into two pieces in time to have a training and testing set. In order to have enough sample for each id, the cut-off timestamp for the training set was defined in \"insampleCutoffTimestamp\" variable as 1650. Figure 4 shows how an instrument is split in time to create training and testing set. While training the model, the training set for each instrument will be fed separately to learn the time patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/data_split.jpg\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        Figure 4: Training and Testing Dataset\n",
    "    \n",
    "In the Kaggle challenge, the metric to evaluate the prediction accuracy was given as Pearson correlation. In statistics, [pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is a measure of the linear correlation between two variables X and Y. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation. It is widely used in the sciences. It was developed by Karl Pearson from a related idea introduced by Francis Galton in the 1880s.\n",
    "\n",
    "Depending on the frequency of the financial data, Pearson correlation (R) can be very small. In finance, given the high ratio of signal-to-noise, even a small R can deliver meaningful value. Please note that the algorithm that won the challenge had only 0.038 R.\n",
    "\n",
    "The following cell includes the code for creating training and testing set, and calculating Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for the output (label)\n",
    "with tf.name_scope('label'):\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1]) \n",
    "# Placeholder to be able to split the data into training and test set while training the network.\n",
    "inSampleCutoff = tf.placeholder(tf.int32, shape = ())\n",
    "\n",
    "# this is important - we only want to train on the in-sample set of rows using TensorFlow\n",
    "y_inSample = y[0:inSampleCutoff]\n",
    "pred_inSample = pred[0:inSampleCutoff]\n",
    "\n",
    "# also extract out of sample predictions and actual values,\n",
    "# we'll use them for evaluation while training the model.\n",
    "y_outOfSample = y[inSampleCutoff:]\n",
    "pred_outOfSample = pred[inSampleCutoff:]\n",
    "\n",
    "with tf.name_scope('stats'):\n",
    "    # Pearson correlation to evaluate the model\n",
    "    covariance = tf.reduce_sum(tf.matmul(tf.transpose(tf.subtract(pred_inSample, tf.reduce_mean(pred_inSample))),tf.subtract(y_inSample, tf.reduce_mean(y_inSample))))\n",
    "    var_pred = tf.reduce_sum(tf.square(tf.subtract(pred_inSample, tf.reduce_mean(pred_inSample))))\n",
    "    var_y = tf.reduce_sum(tf.square(tf.subtract(y_inSample, tf.reduce_mean(y_inSample))))\n",
    "    pearson_corr = covariance / tf.sqrt(var_pred * var_y) \n",
    "\n",
    "tf.summary.scalar(\"pearson_corr\", pearson_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most of the traditional machine learning and deep learning methods, it is assumed that the feature set and predicted value have zero mean and unit variance gaussian distribution. Empirical studies show that the financial data such as asset returns is often not compatible with this assumption. That is why we normalize the \"y\" variable by subtracting its mean and dividing the result by the standard deviation in the following cell. As an exercise, you can also normalize the features and see if you improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset is also created here. We included the code to split the data in the above cell. \n",
    "# The difference is that the above code will be used in the training by the TensorFlow.\n",
    "# This code will not be used by TensorFlow and creates the testing dataset whenever it is executed.\n",
    "dfInSample = df[df.timestamp <  insampleCutoffTimestamp]\n",
    "# create a reference dataframe (that only depends on in-sample data)\n",
    "# that gives us standard deviation and mean information on per-id basis\n",
    "# we'll use it later for variance stabilization\n",
    "meanStdById = dfInSample.groupby(['id']).agg( {'y':['mean', 'std']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the graph for training the model and see intermediate diagnostics results and the final result. We defined the important hyperparameters including the epoch, training batch size and learning rate at the top of the cell. Initially, the epoch is set to 1 because it takes 15-20 minutes to complete the training with 10 epochs even though we are using GPUs. In order to speed up the training in the lab environment, we provided pre-trained networks with 10 epochs and 20 epochs. An adaptive learning rate starting from 0.002 with exponential decay is used for the training from scratch. Learning rate should be set to 0.00058 and 0.00061 for using pre-trained models with 10 and 15 epochs respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "display_step = 100 \n",
    "epoch = 1\n",
    "pre_trained_model = '/dli/tasks/task2/task/SavedModels/model_epoch_10.ckpt'\n",
    "mini_batch_limit = 1300\n",
    "\n",
    "# set up adaptive learning rate:\n",
    "globalStep = tf.placeholder(tf.float32)\n",
    "# Ratio of globalStep / totalDecaySteps is designed to indicate how far we've progressed in training.\n",
    "# the ratio is 0 at the beginning of training and is 1 at the end.\n",
    "# adaptiveLearningRate will thus change from the starting learningRate to learningRate * decay_rate\n",
    "# in order to simplify the code, we are fixing the total number of decay steps at 1 and pass globalStep\n",
    "# as a fraction that starts with 0 and tends to 1.\n",
    "# Learning rate should be set to 0.002 if you are training from scratch.\n",
    "# Learning rate should be set to 0.00058 if you are using the pre-trained network with 10 epochs.\n",
    "# Learning rate should be set to 0.00061 if you are using the pre-trained network with 15 epochs.\n",
    "adaptiveLearningRate = tf.train.exponential_decay(\n",
    "  0.00058,       # Start with this learning rate\n",
    "  globalStep,  # globalStep / totalDecaySteps shows how far we've progressed in training\n",
    "  1,           # totalDecaySteps\n",
    "  0.3)         # decay_rate, the factor by which the starting learning rate will be \n",
    "               # multiplied when the training is finished\n",
    "    \n",
    "# Define loss and optimizer\n",
    "# Note the loss only involves in-sample rows\n",
    "# Regularization is added in the loss function to avoid over-fitting\n",
    "rnn_variables = lstm_variables = [v for v in tf.trainable_variables()\n",
    "                    if v.name.startswith('rnn')]\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.nn.l2_loss(tf.subtract(y_inSample,pred_inSample)) + tf.contrib.layers.apply_regularization(tf.contrib.layers.l2_regularizer(scale=0.0001), tf.trainable_variables())\n",
    "\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=adaptiveLearningRate).minimize (loss) \n",
    "\n",
    "# Getting unique ids to train the network per id basis.\n",
    "ids = df.id.unique()\n",
    "ids.sort()\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "# initialize the variables \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "totalActual = []\n",
    "totalPredicted = []\n",
    "import random\n",
    "# Launch the graph \n",
    "# Implement Cross Validation, but in a vay that preserves temporal structure for id's \n",
    "with tf.Session() as sess:  \n",
    "    # Global variables are initialized\n",
    "    sess.run(init) \n",
    "    \n",
    "    # Restore latest checkpoint\n",
    "    model_saver = tf.train.Saver()\n",
    "    model_saver.restore(sess, pre_trained_model)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"/dli/tasks/tensorboard/logs\", graph=tf.get_default_graph())\n",
    "    step = 50  \n",
    "    writer_step = 1;\n",
    "    for i in range(epoch):\n",
    "        print('Epoch: ', i, '******************************')        \n",
    "        actual = []\n",
    "        predicted = []\n",
    "        \n",
    "        random.shuffle(ids)\n",
    "\n",
    "        for thisId in ids:\n",
    "            # Getting the data of the current id\n",
    "            this_df = df[df.id == thisId].copy()\n",
    "            this_df = this_df.sort_values(['id', 'timestamp'])\n",
    "                        \n",
    "            # we need to pass training set to the graph definition\n",
    "            # optimization will only consider in training set\n",
    "            inSampleSize, _ = this_df[this_df.timestamp < insampleCutoffTimestamp].shape\n",
    "            totalRows, _ = this_df.shape\n",
    "            \n",
    "            batch_y = this_df.loc[:,'y'].values            \n",
    "            batch_x = this_df[colList].values\n",
    "                    \n",
    "            if totalRows < n_time_steps:\n",
    "                continue\n",
    "\n",
    "            # Data is formated as a 3D tensor with the shape of (batch_size, n_time, n_feature) for LSTM\n",
    "            # n_time_steps parameter determines how many steps that LSTM will unroll in time\n",
    "            complete_x = np.zeros([totalRows-n_time_steps+1, n_time_steps, len(colList)])\n",
    "            for n in range(n_time_steps):\n",
    "                complete_x[:,n,:]=batch_x[n:totalRows-n_time_steps+n+1,:]\n",
    "            \n",
    "            batch_y = batch_y[n_time_steps-1:]\n",
    "            inSampleSize -= n_time_steps - 1\n",
    "\n",
    "            # variance stabilizing transform\n",
    "            # some id's will not have in-sample rows, we cannot perform transform on those\n",
    "            # furthermore, since there is not in-sample rows to train on, we must skip\n",
    "            if inSampleSize < 10:\n",
    "                continue\n",
    "                \n",
    "            # perform variance stabilization\n",
    "            thisMean = meanStdById.loc[thisId][0]\n",
    "            thisStd = meanStdById.loc[thisId][1]\n",
    "            batch_y = (batch_y - thisMean) / thisStd\n",
    "            \n",
    "            batch_y = batch_y.reshape(-1,1)\n",
    "            minibatchSize, _ = batch_y.shape\n",
    "\n",
    "            # we want to make sure that RNN reaches steady state\n",
    "            if minibatchSize < mini_batch_limit: \n",
    "                continue \n",
    "            \n",
    "            # Run optimization \n",
    "            # note: keep_prob is set to 0.5 for training only!\n",
    "            _, currentRate = sess.run([optimizer, adaptiveLearningRate], feed_dict={x: complete_x, y: batch_y, keep_prob:0.5, inSampleCutoff:inSampleSize, globalStep:i/epoch})\n",
    "\n",
    "            # Obtain out of sample target variable and our prediction\n",
    "            y_oos, pred_oos = sess.run([y_outOfSample, pred_outOfSample], feed_dict={x: complete_x, y: batch_y, keep_prob:1.0, inSampleCutoff:inSampleSize}) \n",
    "            \n",
    "            # flatten the returned lists\n",
    "            y_oos = [y for x in y_oos for y in x]\n",
    "            pred_oos = [y for x in pred_oos for y in x]\n",
    "            \n",
    "            #reverse transform before recording the results\n",
    "            if inSampleSize:            \n",
    "                y_oos = [ (t*thisStd + thisMean) for t in y_oos]\n",
    "                pred_oos = [ (t*thisStd + thisMean) for t in pred_oos]\n",
    "            \n",
    "            # record the results\n",
    "            actual.extend(y_oos)\n",
    "            predicted.extend(pred_oos)\n",
    "                       \n",
    "            totalActual.extend(y_oos)\n",
    "            totalPredicted.extend(pred_oos)\n",
    "            \n",
    "            # Once every display_step show some diagnostics - the loss function, in-sample correlation, etc.\n",
    "            if step % display_step == 0: \n",
    "                # Calculate batch accuracy \n",
    "                # Calculate batch loss \n",
    "                correl, lossResult, summary = sess.run([pearson_corr, loss, summary_op], feed_dict={x: complete_x, y: batch_y, keep_prob:1.0, inSampleCutoff:inSampleSize})\n",
    "                \n",
    "                writer.add_summary(summary, writer_step)\n",
    "                writer_step += 1\n",
    "                # corrcoef sometimes fails to compute correlation for a perfectly valid reason (e.g. stdev(pred_oos) is 0)\n",
    "                # it sets the result to nan, but also gives an annoying warning\n",
    "                # the following suppresses the warning\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    correl_oos = np.corrcoef(y_oos, pred_oos)[0,1]\n",
    "                    \n",
    "                print('LR: %s - Iter %s, minibatch loss = %s, minibatch corr = %s, oos %s (%s/%s)' % (currentRate, step, lossResult, correl, correl_oos, inSampleSize, totalRows))\n",
    "                \n",
    "            step += 1 \n",
    "       \n",
    "        print('Optimization Finished!') \n",
    "        print('Correl: ', np.corrcoef(actual, predicted)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes 3-5 minutes to complete the training with 1 epochs. We also provided TensorBoard to review the model architecture, loss and correlation variables. TensorBoard is a suite of web applications for inspecting and understanding your TensorFlow runs and graphs. \n",
    "\n",
    "### Click [here](/tensorboard/) to start TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a correlation value around R = 0.04. Note that the correlation tends to increase with each epoch (but not always). \n",
    "\n",
    "Let's use the pre-trained model with 15 epochs by setting the pre_trained_model variable as **pre_trained_model = '/dli/tasks/task2/task/SavedModels/model_epoch_15.ckpt'** in above cell, lower the starting Learning Rate to **0.00061** and re-run everything using Kernel->Restart & Run All.\n",
    "\n",
    "What is the correlation that you get this time? Was it improved?\n",
    "\n",
    "Since training takes significant amount of time, we recommend you train the model with 20 epochs and check the correlation after this lab in your environment. You should get a correlation value around R = 0.05.\n",
    "\n",
    "## Optional Exercise\n",
    "Please read ahead and only come back to these optional exercises if time permits.\n",
    "\n",
    "**Train from scratch** [20-30 mins]\n",
    "\n",
    "First, change the # of epochs to 20 in the above cell. Second, put the starting learning rate back to **0.002**. Third, comment out the two line where the pre-trained model is loaded (under \"Restore latest checkpoint\"). Then re-run everything using Kernel->Restart & Run All. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can a portfolio manager assess the predicted signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could scatter-plot actual returns over the predicted returns, however correlation is not visually apparent on scatter plots when the correlation is below 20-30%. The correlation we achieve in this signal is much weaker which is typical of modern financial markets. Correlations which we often observe in other applications of predictive models are all but impossible in the financial markets which are highly efficient (simply put, unpredictable). If we imagine that someone has a signal with correlation of 30% using leverage the person would soon get extremely rich - and the observed signal (inefficiency) would disappear from the market.\n",
    "\n",
    "In order to visually assess the signal, we split out of sample data points into buckets based on the value of predicted returns. We then compute per-bucket mean actual returns. Then we plot mean actual returns (Y axis) against predicted returns (X axis). We thus plot one point per bucket. By taking mean value, we average out the variance within each bucket and uncover the predictive value of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = totalActual\n",
    "predicted = totalPredicted\n",
    "\n",
    "actualMeanReturn = []\n",
    "predictedMeanReturn = []\n",
    "stdActualReturns = []\n",
    "# Buckets are created\n",
    "buckets = np.arange(-0.02,0.02,0.002)\n",
    "\n",
    "actual = np.array(actual)\n",
    "predicted = np.array(predicted)\n",
    "\n",
    "# Predicted values and the actual values are placed into buckets\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for i in range(len(buckets)-1):\n",
    "        index = np.logical_and(predicted>buckets[i], predicted<buckets[i+1])\n",
    "        thisBucket = actual[index].mean()\n",
    "        actualMeanReturn.append(thisBucket)\n",
    "        predictedMeanReturn.append(predicted[index].mean())\n",
    "        stdActualReturns.append(actual[index].std())\n",
    "\n",
    "# Actual versus predicted values are plotted\n",
    "plt.figure()\n",
    "plt.plot(predictedMeanReturn,actualMeanReturn, marker='*')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(predictedMeanReturn, actualMeanReturn, yerr = stdActualReturns, marker='*')\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How much variance is there?**\n",
    "\n",
    "Plot 2 answers this question by adding error bars to the previous plot. Length of the error bar is equal to the standard deviation of actual returns within each respective bucket.\n",
    "Plots such as these would be typically used by a portfolio manager to assess behavior of prospective signals and to assess signal levels at which an action should be taken. The simplest trading system utilizing this signal would buy security when predicted return is above some threshold (say, above 0.5%) and sell (or short-sell) the security when the signal is below negative threshold (e.g. below -0.5%). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following steps after the lab.\n",
    "\n",
    "1. Try using other machine learning techniques such as random forest, ridge regression, xgboost and compare the correlation with LSTM based predictor.\n",
    "\n",
    "2. Try using autoencoder to extract fewer features than the original dataset provides and use the features as input to the deep learning model. Analyze the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, step by step implementation of a LSTM based deep neural network to predict time series financial data is presented. The performance of the model is evaluated with the pearson correlation and competitive performance is achieved. The code provided in this lab can be used in complex trading strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Post-Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, don't forget to save your work from this lab before time runs out and the instance shuts down!!\n",
    "\n",
    "1. You can download the data from this [link](https://www.kaggle.com/c/two-sigma-financial-modeling).\n",
    "\n",
    "2. To use the data, please set the \"usePreparedData\" variable to False before running the code on your environment.\n",
    "\n",
    "3. Also, remove the code \"model_saver.restore(sess, pre_trained_model)\" to train the model for you data.\n",
    "\n",
    "4. You can execute the following cell block to zip the files you've been working on, and download it from the link below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -cvf output.zip \"Trading Strategy for Finance using LSTMs.ipynb\" prepareData.py images/dnn.jpg images/data.jpg images/data_split.jpg images/rnn.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Download output.zip](output.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
